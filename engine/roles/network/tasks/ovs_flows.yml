---
# OVS flow configuration

# make sure openvswitch is installed
- name: check ovs is available
  command: which ovs-vsctl

- name: clear all ovs flows
  command: ovs-ofctl del-flows ovs0

- name: default ovs drop flow
  command: ovs-ofctl mod-flows ovs0 priority=0,actions=drop

- name: store ovs flows in a file
  copy:
    content: "{{ flows | tojson }}"
    dest: "{{ remote_flow_file }}"

- name: create ovs flows
  script: files/ovs_flows.py --flowfile {{ remote_flow_file }} --vlanid {{ iface_vlan_id }} --node {{ node }}
  register: flow_ifaces_raw

- name: set flow interfaces
  set_fact:
    flow_ifaces: '{{ flow_ifaces_raw.stdout_lines[-1] }}'

- name: set used raw interfaces
  set_fact:
    use_ifaces: '{{ flow_ifaces_raw.stdout_lines[-2] }}'

- name: copy flow cgroup priority script to node
  template:
    src: 'cgroup_priorities_flow.sh'
    dest: '{{ data_path }}/cgroup_priorities_flow.sh'
    mode: '0755'

- name: execute flow cgroup priority script
  command: 'bash {{ data_path }}/cgroup_priorities_flow.sh'

# disable all hardware interfaces, only enable the ones used in a flow
# -> prevent message leaking to other topologies running concurrently in the testbed
- name: disable all hardware interfaces
  command: ip link set {{ item.value.name }} down
  loop: '{{ node_ifaces | dict2items }}'

- name: enable used flow hardware interfaces
  command: ip link set {{ item.value.name }} up
  when: (item.key | string ) in use_ifaces
  loop: '{{ node_ifaces | dict2items }}'

- name: check if low latency kernel used when real-time priority for IRQs specified
  fail: 
    msg: IRQs cannot be set to real-time priority without the low-latency kernel!
  when: (network[experiment.network].nic_irq_rt is defined and network[experiment.network].nic_irq_rt) and ((use_low_latency_kernel is defined and not use_low_latency_kernel | bool) or use_low_latency_kernel is not defined)


# Prepare the NIC IRQs for real-time priority
- name: set real-time priority for used interfaces
  shell: 'top -b -n 1 | pgrep {{ item.value.name }} | while read -r p; do chrt -p 99 $p; done'
  loop: '{{ node_ifaces | dict2items }}'
  when: (item.key | string ) in use_ifaces and use_low_latency_kernel is defined and use_low_latency_kernel | bool and network[experiment.network].nic_irq_rt is defined and network[experiment.network].nic_irq_rt

# TODO change here to only use the ifaces which are supported
- debug:
    msg: "Used interfaces {{ node_ifaces | dict2items }}"
# Pin NIC IRQs to the first two of the isolated CPUS
- name: set affinity for IRQs related to the used interfaces
  # shell: 'ls /sys/class/net/{{ item.value.name }}/device/msi_irqs/ | while read -r p; do printf "%x" $((1 << $p%5)) > /proc/irq/$p/smp_affinity; done'
  shell: 'ls /sys/class/net/{{ item.value.name }}/device/msi_irqs/ | while read -r p; do printf "%x" $((1 << $p% {{ network[experiment.network].num_net_cores }})) > /proc/irq/$p/smp_affinity; done'
  loop: '{{ node_ifaces | dict2items }}'
  when: (item.key | string ) in use_ifaces and network[experiment.network].num_net_cores is defined and network[experiment.network].num_net_cores > 0 # and network[experiment.network].num_net_cores <= num_isolated_cores
